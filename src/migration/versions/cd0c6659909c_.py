"""empty message

Revision ID: cd0c6659909c
Revises: 6e9f0046c209
Create Date: 2025-11-07 18:12:37.215753

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = 'cd0c6659909c'
down_revision = '6e9f0046c209'
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('app_dataset_join',
    sa.Column('id', sa.UUID(), server_default=sa.text('uuid_generate_v4()'), nullable=False),
    sa.Column('app_id', sa.UUID(), nullable=False),
    sa.Column('dataset_id', sa.UUID(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_app_dataset_join_id'),
    sa.UniqueConstraint('app_id', 'dataset_id', name='uk_app_dataset_join_app_dataset')
    )
    with op.batch_alter_table('app_dataset_join', schema=None) as batch_op:
        batch_op.create_index('idx_app_dataset_join_app_id', ['app_id'], unique=False)
        batch_op.create_index('idx_app_dataset_join_dataset_id', ['dataset_id'], unique=False)

    op.create_table('dataset',
    sa.Column('id', sa.UUID(), server_default=sa.text('uuid_generate_v4()'), nullable=False),
    sa.Column('account_id', sa.UUID(), nullable=False),
    sa.Column('name', sa.String(length=255), server_default=sa.text("''::character varying"), nullable=False),
    sa.Column('icon', sa.String(length=255), server_default=sa.text("''::character varying"), nullable=False),
    sa.Column('description', sa.Text(), server_default=sa.text("''::text"), nullable=False),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_dataset_id')
    )
    with op.batch_alter_table('dataset', schema=None) as batch_op:
        batch_op.create_index('idx_dataset_account_id', ['account_id'], unique=False)

    op.create_table('dataset_query',
    sa.Column('id', sa.UUID(), server_default=sa.text('uuid_generate_v4()'), nullable=False),
    sa.Column('dataset_id', sa.UUID(), nullable=False),
    sa.Column('query', sa.Text(), server_default=sa.text("''::text"), nullable=False),
    sa.Column('source', sa.String(length=255), server_default=sa.text("'HitTesting'::character varying"), nullable=False),
    sa.Column('source_app_id', sa.UUID(), nullable=True),
    sa.Column('created_by', sa.UUID(), nullable=True),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_dataset_query_id')
    )
    with op.batch_alter_table('dataset_query', schema=None) as batch_op:
        batch_op.create_index('idx_dataset_query_created_by', ['created_by'], unique=False)
        batch_op.create_index('idx_dataset_query_dataset_id', ['dataset_id'], unique=False)
        batch_op.create_index('idx_dataset_query_source_app_id', ['source_app_id'], unique=False)

    op.create_table('document',
    sa.Column('id', sa.UUID(), server_default=sa.text('uuid_generate_v4()'), nullable=False),
    sa.Column('account_id', sa.UUID(), nullable=False),
    sa.Column('dataset_id', sa.UUID(), nullable=False),
    sa.Column('upload_file_id', sa.UUID(), nullable=False),
    sa.Column('process_rule_id', sa.UUID(), nullable=False),
    sa.Column('batch', sa.String(length=255), server_default=sa.text("''::character varying"), nullable=False),
    sa.Column('name', sa.String(length=255), server_default=sa.text("''::character varying"), nullable=False),
    sa.Column('position', sa.Integer(), server_default=sa.text('1'), nullable=False),
    sa.Column('character_count', sa.Integer(), server_default=sa.text('0'), nullable=False),
    sa.Column('token_count', sa.Integer(), server_default=sa.text('0'), nullable=False),
    sa.Column('processing_started_at', sa.DateTime(), nullable=True),
    sa.Column('parsing_completed_at', sa.DateTime(), nullable=True),
    sa.Column('splitting_completed_at', sa.DateTime(), nullable=True),
    sa.Column('indexing_completed_at', sa.DateTime(), nullable=True),
    sa.Column('completed_at', sa.DateTime(), nullable=True),
    sa.Column('stopped', sa.DateTime(), nullable=True),
    sa.Column('error', sa.Text(), server_default=sa.text("''::text"), nullable=False),
    sa.Column('enabled', sa.Boolean(), server_default=sa.text('false'), nullable=False),
    sa.Column('disabled_at', sa.DateTime(), nullable=True),
    sa.Column('status', sa.String(length=255), server_default=sa.text("'waiting'::character varying"), nullable=False),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_document_id')
    )
    with op.batch_alter_table('document', schema=None) as batch_op:
        batch_op.create_index('idx_document_account_id', ['account_id'], unique=False)
        batch_op.create_index('idx_document_batch', ['batch'], unique=False)
        batch_op.create_index('idx_document_dataset_id', ['dataset_id'], unique=False)
        batch_op.create_index('idx_document_process_rule_id', ['process_rule_id'], unique=False)
        batch_op.create_index('idx_document_status', ['status'], unique=False)
        batch_op.create_index('idx_document_upload_file_id', ['upload_file_id'], unique=False)

    op.create_table('keyword_table',
    sa.Column('id', sa.UUID(), server_default=sa.text('uuid_generate_v4()'), nullable=False),
    sa.Column('dataset_id', sa.UUID(), nullable=False),
    sa.Column('keyword_table', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), nullable=False),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_keyword_table_id')
    )
    with op.batch_alter_table('keyword_table', schema=None) as batch_op:
        batch_op.create_index('idx_keyword_table_dataset_id', ['dataset_id'], unique=False)

    op.create_table('process_rule',
    sa.Column('id', sa.UUID(), server_default=sa.text('uuid_generate_v4()'), nullable=False),
    sa.Column('account_id', sa.UUID(), nullable=False),
    sa.Column('dataset_id', sa.UUID(), nullable=False),
    sa.Column('mode', sa.String(length=255), server_default=sa.text("'automic'::character varying"), nullable=False),
    sa.Column('rule', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), nullable=False),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_process_rule_id')
    )
    with op.batch_alter_table('process_rule', schema=None) as batch_op:
        batch_op.create_index('idx_process_rule_account_id', ['account_id'], unique=False)
        batch_op.create_index('idx_process_rule_dataset_id', ['dataset_id'], unique=False)

    op.create_table('segment',
    sa.Column('id', sa.UUID(), server_default=sa.text('uuid_generate_v4()'), nullable=False),
    sa.Column('account_id', sa.UUID(), nullable=False),
    sa.Column('dataset_id', sa.UUID(), nullable=False),
    sa.Column('document_id', sa.UUID(), nullable=False),
    sa.Column('node_id', sa.UUID(), nullable=False),
    sa.Column('position', sa.Integer(), server_default=sa.text('1'), nullable=False),
    sa.Column('content', sa.Text(), server_default=sa.text("''::text"), nullable=False),
    sa.Column('character_count', sa.Integer(), server_default=sa.text('0'), nullable=False),
    sa.Column('token_count', sa.Integer(), server_default=sa.text('0'), nullable=False),
    sa.Column('keywords', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'[]'::jsonb"), nullable=False),
    sa.Column('hash', sa.String(length=255), server_default=sa.text("''::character varying"), nullable=False),
    sa.Column('hit_count', sa.Integer(), server_default=sa.text('0'), nullable=False),
    sa.Column('enabled', sa.Boolean(), server_default=sa.text('false'), nullable=False),
    sa.Column('disabled_at', sa.DateTime(), nullable=True),
    sa.Column('processing_started_at', sa.DateTime(), nullable=True),
    sa.Column('indexing_completed_at', sa.DateTime(), nullable=True),
    sa.Column('completed_at', sa.DateTime(), nullable=True),
    sa.Column('stopped', sa.DateTime(), nullable=True),
    sa.Column('error', sa.Text(), server_default=sa.text("''::text"), nullable=False),
    sa.Column('status', sa.String(length=255), server_default=sa.text("'waiting'::character varying"), nullable=False),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('CURRENT_TIMESTAMP(0)'), nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_segment_id'),
    sa.UniqueConstraint('hash', name='uk_segment_hash')
    )
    with op.batch_alter_table('segment', schema=None) as batch_op:
        batch_op.create_index('idx_segment_account_id', ['account_id'], unique=False)
        batch_op.create_index('idx_segment_dataset_id', ['dataset_id'], unique=False)
        batch_op.create_index('idx_segment_document_id', ['document_id'], unique=False)
        batch_op.create_index('idx_segment_enabled', ['enabled'], unique=False)
        batch_op.create_index('idx_segment_node_id', ['node_id'], unique=False)
        batch_op.create_index('idx_segment_status', ['status'], unique=False)

    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('segment', schema=None) as batch_op:
        batch_op.drop_index('idx_segment_status')
        batch_op.drop_index('idx_segment_node_id')
        batch_op.drop_index('idx_segment_enabled')
        batch_op.drop_index('idx_segment_document_id')
        batch_op.drop_index('idx_segment_dataset_id')
        batch_op.drop_index('idx_segment_account_id')

    op.drop_table('segment')
    with op.batch_alter_table('process_rule', schema=None) as batch_op:
        batch_op.drop_index('idx_process_rule_dataset_id')
        batch_op.drop_index('idx_process_rule_account_id')

    op.drop_table('process_rule')
    with op.batch_alter_table('keyword_table', schema=None) as batch_op:
        batch_op.drop_index('idx_keyword_table_dataset_id')

    op.drop_table('keyword_table')
    with op.batch_alter_table('document', schema=None) as batch_op:
        batch_op.drop_index('idx_document_upload_file_id')
        batch_op.drop_index('idx_document_status')
        batch_op.drop_index('idx_document_process_rule_id')
        batch_op.drop_index('idx_document_dataset_id')
        batch_op.drop_index('idx_document_batch')
        batch_op.drop_index('idx_document_account_id')

    op.drop_table('document')
    with op.batch_alter_table('dataset_query', schema=None) as batch_op:
        batch_op.drop_index('idx_dataset_query_source_app_id')
        batch_op.drop_index('idx_dataset_query_dataset_id')
        batch_op.drop_index('idx_dataset_query_created_by')

    op.drop_table('dataset_query')
    with op.batch_alter_table('dataset', schema=None) as batch_op:
        batch_op.drop_index('idx_dataset_account_id')

    op.drop_table('dataset')
    with op.batch_alter_table('app_dataset_join', schema=None) as batch_op:
        batch_op.drop_index('idx_app_dataset_join_dataset_id')
        batch_op.drop_index('idx_app_dataset_join_app_id')

    op.drop_table('app_dataset_join')
    # ### end Alembic commands ###
